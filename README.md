# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains data about bank telemarketing. The dataset has 21 columns and about 32000 rows. We're trying to predict the column named 'y' which has two class possibilities (yes, no). There is heavy class imbalance in the dataset. There are less than 3200 'yes' records which accounts to around 10% of the total rows.


The best performing model was the logistic regression model built manually and then used hyperdrive to tune the parameters. The accuracy of this model was 91.85%

## Scikit-learn Pipeline

The training pipeline for SKLearn model is as follows:
* Load the data from TabularDatasetFactory
* Clean the dataset using predefined function named **clean_data**
  * One Hot Encode the categorical columns
  * Separate the features and target
* Split the dataset into train and test sets
* Build a Logistic Regression model
* Tune hyperparameters using Hyperdrive
* Select the best model from the tuning
* Save and register the model for the later use 

**What are the benefits of the parameter sampler you chose?**

I have chosen Random Sampling to choose the hyperparameters. This type of sampling is good for initial experiments and we can refine our search space in later experiments. 

Random Sampling
* Supports discrete and continuous hyperparameters
* Supports early termination of low performance runs


I choose Bandit Policy for early termination. This policy takes **slack_factor or slack_amount** as an input and any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated.

Example:

Consider a Bandit policy with slack_amount = 0.1 and evaluation_interval = 100. If Run 3 is the currently best performing run with an AUC (performance metric) of 0.8 after 100 intervals, then any run with an AUC less than 0.7 (0.8 - 0.1) after 100 iterations will be terminated. Similarly, the delay_evaluation can also be used to delay the first termination policy evaluation for a specific number of sequences.

## AutoML

The best model generated by AutoML is a VotingEnsemble classifier. This is built using engineered features and used LightGBM as the model. This model gave the best accuracy of 91.7% among all the other models generated. Before the model building, the AutoML also performed some data checks, where it found the class imbalance in our dataset. It balanced the dataset before going to model building.

The pipeline for the best model is as follows:

* Separate Numerical and Categorical columns
* Impute all the numerical columns with the mean value of each column
* The categorical columns were then Label Encoded and Vectorized using Count Vectorizer
* Then that dataset is sent to the model for training

Note: I removed the preprocessing step that was there in the sklearn pipeline to see how AutoML encodes/deals with different columns.

## Pipeline comparison

* In SKLearn pipeline, we did not do data quality checks like imbalance in the classes etc., Whereas the AutoML did that and balanced the dataset. 
* In SKLearn pipeline, we only built a single model and tuned the hyperparameters for that model whereas in AutoML, eventhough there's a time constraint it was able to build around 20 models in 30 mins.
* The difference in accuracy between both the models is very less (0.1%). The AutoML's best model gave an accuracy of 91.7 while the logistic gave 91.8%. The reason for logistic to give higher could be class imbalance. We didn't treat the class imbalance in our dataset in the manual pipeline. So, the model is biased towards the negtive class.

## Future work

As the dataset is not complex and a linear model like Logistic is able to learn the data, we can work on improving this model. In terms os data preprocessing, treating class imbalance would be a major thing to try out and later we can experiment with different imputing techniques instead of mean (like nearest). There's probably some improvements that can be done in hyperparameters also. Giving a wider search space and diffrenet sampling technique like Bayesian Sampling.

As always, we can also use AutoML to comeup with the best model by giving it more time and compute resources. 

## Proof of cluster clean up

I have used my personal Azure account for this project and deleted the clusters as soon as the project is done. So, by the time I started this writeup, the cluster has been deleted.

